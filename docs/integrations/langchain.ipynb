{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586226e7",
   "metadata": {},
   "source": [
    "# Evaluating Langchain QA Chains\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. It can also be used to create RAG systems (or QA systems as they are reffered to in langchain). If you want to know more about creating RAG systems with langchain you can check the [docs](https://python.langchain.com/docs/use_cases/question_answering/).\n",
    "\n",
    "With this integration you can easily evaluate your QA chains with the metrics offered in ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb5deb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach to the existing event loop when using jupyter notebooks\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e32dc",
   "metadata": {},
   "source": [
    "First lets load the dataset. We are going to build a generic QA system over the [NYC wikipedia page](https://en.wikipedia.org/wiki/New_York_City). Load the dataset and create the `VectorstoreIndex` and the `RetrievalQA` from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa9a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "loader = TextLoader(\"./nyc_wikipedia/nyc_text.txt\")\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm, retriever=index.vectorstore.as_retriever(), return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ebdf8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New York City was named in honor of the Duke of York, who would become King James II of England. King Charles II appointed the Duke as proprietor of the former territory of New Netherland, including the city of New Amsterdam, when England seized it from Dutch control.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing it out\n",
    "\n",
    "question = \"How did New York City get its name?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748787c1",
   "metadata": {},
   "source": [
    "Now in order to evaluate the qa system we generated a few relevant questions. We've generated a few question for you but feel free to add any you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67ce0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = [\n",
    "    \"What is the population of New York City as of 2020?\",\n",
    "    \"Which borough of New York City has the highest population?\",\n",
    "    \"What is the economic significance of New York City?\",\n",
    "    \"How did New York City get its name?\",\n",
    "    \"What is the significance of the Statue of Liberty in New York City?\",\n",
    "]\n",
    "\n",
    "queries = [{\"query\": q} for q in eval_questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b7e2c4",
   "metadata": {},
   "source": [
    "### Introducing `RagasEvaluatorChain`\n",
    "\n",
    "`RagasEvaluatorChain` creates a wrapper around the metrics ragas provides (documented [here](https://github.com/explodinggradients/ragas/blob/main/docs/metrics.md)), making it easier to run these evaluation with langchain and langsmith.\n",
    "\n",
    "The evaluator chain has the following APIs\n",
    "\n",
    "- `__call__()`: call the `RagasEvaluatorChain` directly on the result of a QA chain.\n",
    "- `evaluate()`: evaluate on a list of examples (with the input queries) and predictions (outputs from the QA chain). \n",
    "- `evaluate_run()`: method implemented that is called by langsmith evaluators to evaluate langsmith datasets.\n",
    "\n",
    "lets see each of them in action to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d9266d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.langchain.evalchain import RagasEvaluatorChain\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_relevancy\n",
    "\n",
    "# create evaluation chains\n",
    "faithfulness_chain = RagasEvaluatorChain(metric=faithfulness)\n",
    "answer_rel_chain = RagasEvaluatorChain(metric=answer_relevancy)\n",
    "context_rel_chain = RagasEvaluatorChain(metric=context_relevancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb95467",
   "metadata": {},
   "source": [
    "1. `__call__()`\n",
    "\n",
    "Directly run the evaluation chain with the results from the QA chain. Do note that metrics like context_relevancy and faithfulness require the `source_documents` to be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ede32cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result = faithfulness_chain(result)\n",
    "eval_result[\"faithfulness_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11295b5",
   "metadata": {},
   "source": [
    "2. `evaluate()`\n",
    "\n",
    "Evaluate a list of inputs/queries and the outputs/predictions from the QA chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ce7bff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 1/1 [00:38<00:00, 38.77s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'faithfulness_score': 1.0},\n",
       " {'faithfulness_score': 0.5},\n",
       " {'faithfulness_score': 0.75},\n",
       " {'faithfulness_score': 1.0},\n",
       " {'faithfulness_score': 1.0}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the queries as a batch for efficiency\n",
    "predictions = qa_chain.batch(queries)\n",
    "\n",
    "# evaluate\n",
    "print(\"evaluating...\")\n",
    "r = faithfulness_chain.evaluate(queries, predictions)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc71587",
   "metadata": {},
   "source": [
    "## Evaluate with langsmith\n",
    "\n",
    "[Langsmith](https://docs.smith.langchain.com/) is a platform that helps to debug, test, evaluate and monitor chains and agents built on any LLM framework. It also seamlessly integrates with LangChain. \n",
    "\n",
    "Langsmith also has a tools to build a testing dataset and run evaluations against them and with `RagasEvaluatorChain` you can use the ragas metrics for running langsmith evaluations as well. To know more about langsmith evaluations checkout the [quickstart](https://docs.smith.langchain.com/evaluation/quickstart).\n",
    "\n",
    "\n",
    "Lets start of creating the dataset with the NYC questions listed in `eval_questions`. Create a new langsmith dataset and upload the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e75144c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using existing dataset:  NYC test\n"
     ]
    }
   ],
   "source": [
    "# dataset creation\n",
    "\n",
    "from langsmith import Client\n",
    "from langsmith.utils import LangSmithError\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"NYC test\"\n",
    "\n",
    "try:\n",
    "    # check if dataset exists\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    print(\"using existing dataset: \", dataset.name)\n",
    "except LangSmithError:\n",
    "    # if not create a new one with the generated query examples\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name, description=\"NYC test dataset\"\n",
    "    )\n",
    "    for q in eval_questions:\n",
    "        client.create_example(\n",
    "            inputs={\"query\": q},\n",
    "            dataset_id=dataset.id,\n",
    "        )\n",
    "\n",
    "    print(\"Created a new dataset: \", dataset.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0181dac",
   "metadata": {},
   "source": [
    "![](./assets/langsmith-dataset.png)\n",
    "\n",
    "As you can see the questions have been uploaded. Now you can run your QA chain against this test dataset and compare the results in the langchain platform. \n",
    "\n",
    "Before you call `run_on_dataset` you need a factory function which creates a new instance of the QA chain you want to test. This is so that the internal state is not reused when running against each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a6decc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# factory function that return a new qa chain\n",
    "def create_qa_chain(return_context=True):\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=index.vectorstore.as_retriever(),\n",
    "        return_source_documents=return_context,\n",
    "    )\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470ddc97",
   "metadata": {},
   "source": [
    "Now lets run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25f7992f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project '2023-08-22-19-28-17-RetrievalQA' at:\n",
      "https://smith.langchain.com/projects/p/2133d672-b69a-4091-bc96-a4e39d150db5?eval=true\n"
     ]
    }
   ],
   "source": [
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    custom_evaluators=[faithfulness_chain, answer_rel_chain, context_rel_chain],\n",
    "    prediction_key=\"result\",\n",
    ")\n",
    "\n",
    "result = run_on_dataset(\n",
    "    client,\n",
    "    dataset_name,\n",
    "    create_qa_chain,\n",
    "    evaluation=evaluation_config,\n",
    "    input_mapper=lambda x: x,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64bb0c4",
   "metadata": {},
   "source": [
    "You can follow the link to open the result for the run in langsmith. Check out the scores for each example too\n",
    "\n",
    "![](./assets/langsmith-evaluation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125857c9",
   "metadata": {},
   "source": [
    "Now if you want to dive more into the reasons for the scores and how to improve them, click on any example and open the feedback tab. This will show you each scores.\n",
    "\n",
    "![](./assets/langsmith-feedback.png)\n",
    "\n",
    "You can also see the curresponding `RagasEvaluatorChain` trace too to figure out why ragas scored the way it did.\n",
    "\n",
    "![](./assets/langsmith-ragas-chain-trace.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

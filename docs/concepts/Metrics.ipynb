{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8e5d278",
   "metadata": {},
   "source": [
    "## Reference free evaluation\n",
    "\n",
    "In a Retrival augmented generation system it can be time consuming to create ground truth for every set of sample to be analyzed and evaluated. Moreover intelligent insights about the performance of RAG pipeline can be derived with using questions, retrived contexts and generated answer. **Reference free evaluation metrics are the set of metrics can be used without the need of a annotated ground truth**.These metrics can be used against any set of random sample drawn out of a RAG pipeline. \n",
    "\n",
    "\n",
    "\n",
    "1. Faithfulness \n",
    "2. Answer Relevancy \n",
    "3. Context Precision\n",
    "4. Aspect critique \n",
    "\n",
    "\n",
    "## Grouth truth\n",
    "\n",
    "Some performance of some components in a RAG system can only be fully determined using a evaluation dataset that contains annotated ground truth. TODO: Give reference to testset generation\n",
    "\n",
    "1. Context Recall\n",
    "2. Answer semantic similarity\n",
    "3. Answer correctness\n",
    "\n",
    "\n",
    "## Test set generation \n",
    "TODO: Discuss the idea of continual testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bae9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
